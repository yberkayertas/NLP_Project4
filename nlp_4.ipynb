{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HABNo5iUyhjJ"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZxMxUgikMal"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d-txXpysh_ig",
        "outputId": "94e09fc7-5f49-40fb-c94b-f79a2f370b96"
      },
      "outputs": [],
      "source": [
        "!wget http://www.manythings.org/anki/tur-eng.zip\n",
        "!unzip tur-eng.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-RfX7_liBgd"
      },
      "source": [
        "# Preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k3MEFiiFyi34",
        "outputId": "56e0a978-8693-42e4-9aa3-5562d2912c73"
      },
      "outputs": [],
      "source": [
        "!pip install bert-score pandas\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "APLWL5h9iErh",
        "outputId": "21a619e2-f075-42e0-c59d-d9ecc1a287bc"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import unicodedata\n",
        "\n",
        "# Veriyi oku\n",
        "df = pd.read_csv('tur.txt', sep='\\t', names=['en', 'tr', 'attribution'])\n",
        "\n",
        "# Başlangıç için ilk 50.000 satırı alalım (hızlı deneme için)\n",
        "df = df.iloc[:50000]\n",
        "\n",
        "def preprocess(text):\n",
        "    text = text.lower().strip()\n",
        "    text = re.sub(r\"([?.!,¿])\", r\" \\1 \", text) # Noktalamayı ayır\n",
        "    text = re.sub(r'[\" \"]+', \" \", text)\n",
        "    text = re.sub(r\"[^a-zA-ZçğışöüÇĞİŞÖÜ?.!,¿]+\", \" \", text)\n",
        "    return text.strip()\n",
        "\n",
        "df['en'] = df['en'].apply(preprocess)\n",
        "df['tr'] = df['tr'].apply(preprocess)\n",
        "\n",
        "print(df.sample(5)) # Rastgele 5 örnek gör"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PmeHLNUkKbf"
      },
      "source": [
        "# Tokenization and Vocabulary Creation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iVhOP0p_kwQB",
        "outputId": "30e36f85-769b-470d-e6c9-cd45b3aadcc1"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "class Vocab:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {\"<pad>\": 0, \"<sos>\": 1, \"<eos>\": 2, \"<unk>\": 3}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {0: \"<pad>\", 1: \"<sos>\", 2: \"<eos>\", 3: \"<unk>\"}\n",
        "        self.n_words = 4  # Başlangıçtaki özel token sayısı\n",
        "\n",
        "    def add_sentence(self, sentence):\n",
        "        for word in sentence.split():\n",
        "            self.add_word(word)\n",
        "\n",
        "    def add_word(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1\n",
        "\n",
        "# Sözlükleri nesne olarak oluşturalım\n",
        "input_lang = Vocab(\"en\")\n",
        "output_lang = Vocab(\"tr\")\n",
        "\n",
        "# Veri setindeki tüm cümleleri sözlüğe ekleyelim\n",
        "for i, row in df.iterrows():\n",
        "    input_lang.add_sentence(row['en'])\n",
        "    output_lang.add_sentence(row['tr'])\n",
        "\n",
        "print(f\"İngilizce Sözlük Boyutu: {input_lang.n_words}\")\n",
        "print(f\"Türkçe Sözlük Boyutu: {output_lang.n_words}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grKUD6aKlDnm"
      },
      "source": [
        "# Dataset, Padding ve DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4KP5S35NlFiL",
        "outputId": "1130d7bd-b4e6-4d41-f743-2deefc6abc4d"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, df, input_vocab, output_vocab):\n",
        "        self.df = df\n",
        "        self.input_vocab = input_vocab\n",
        "        self.output_vocab = output_vocab\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Cümleleri al ve sayısal listeye çevir\n",
        "        src_sent = self.df.iloc[idx]['en']\n",
        "        trg_sent = self.df.iloc[idx]['tr']\n",
        "\n",
        "        # Başlangıç ve bitiş tokenlarını ekleyerek listeye çeviriyoruz\n",
        "        src_indices = [self.input_vocab.word2index.get(word, 3) for word in src_sent.split()]\n",
        "        trg_indices = [self.output_vocab.word2index.get(word, 3) for word in trg_sent.split()]\n",
        "\n",
        "        src_tensor = torch.tensor([1] + src_indices + [2])\n",
        "        trg_tensor = torch.tensor([1] + trg_indices + [2])\n",
        "\n",
        "        return src_tensor, trg_tensor\n",
        "\n",
        "# Farklı uzunluktaki cümleleri aynı boyuta getirir\n",
        "def collate_fn(batch):\n",
        "    src_batch, trg_batch = zip(*batch)\n",
        "    src_batch = pad_sequence(src_batch, padding_value=0) # <pad> = 0\n",
        "    trg_batch = pad_sequence(trg_batch, padding_value=0)\n",
        "    return src_batch, trg_batch\n",
        "\n",
        "# Dataset ve DataLoaderı oluştur\n",
        "dataset = TranslationDataset(df, input_lang, output_lang)\n",
        "train_loader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "# Kontrol\n",
        "src_sample, trg_sample = next(iter(train_loader))\n",
        "print(f\"Kaynak (EN) Batch Boyutu: {src_sample.shape}\") # [Max_Len, Batch_Size]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOEHwwGBlXom"
      },
      "source": [
        "# Model 1: Vanilla GRU Encoder-Decoder Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Em5srfZalZr8"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "# --- ENCODER ---\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "        self.rnn = nn.GRU(emb_dim, hid_dim, n_layers, dropout=dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src):\n",
        "        # src = [src_len, batch_size]\n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        outputs, hidden = self.rnn(embedded)\n",
        "        # hidden = [n_layers, batch_size, hid_dim]\n",
        "        return hidden\n",
        "\n",
        "# --- DECODER ---\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
        "        super().__init__()\n",
        "        self.output_dim = output_dim\n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        self.rnn = nn.GRU(emb_dim, hid_dim, n_layers, dropout=dropout)\n",
        "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        # input = [batch_size]\n",
        "        input = input.unsqueeze(0) # [1, batch_size]\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        output, hidden = self.rnn(embedded, hidden)\n",
        "        prediction = self.fc_out(output.squeeze(0))\n",
        "        return prediction, hidden\n",
        "\n",
        "# --- SEQ2SEQ  ---\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
        "        # src = [src_len, batch_size], trg = [trg_len, batch_size]\n",
        "        trg_len = trg.shape[0]\n",
        "        batch_size = trg.shape[1]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "\n",
        "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
        "        hidden = self.encoder(src)\n",
        "\n",
        "        input = trg[0,:] # İlk input <sos> tokenı\n",
        "\n",
        "        for t in range(1, trg_len):\n",
        "            output, hidden = self.decoder(input, hidden)\n",
        "            outputs[t] = output\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            top1 = output.argmax(1)\n",
        "            input = trg[t] if teacher_force else top1\n",
        "\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k5qOF5iol35N",
        "outputId": "a75e6478-245c-47dd-813f-79de7bd536d0"
      },
      "outputs": [],
      "source": [
        "# Modeli başlatmak için parametreleri tanımlıyoruz\n",
        "INPUT_DIM = input_lang.n_words\n",
        "OUTPUT_DIM = output_lang.n_words\n",
        "ENC_EMB_DIM = 256\n",
        "DEC_EMB_DIM = 256\n",
        "HID_DIM = 512\n",
        "N_LAYERS = 2 # Vanilla modellerde 2 katman iyidir\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5\n",
        "\n",
        "# GPU kontrolü\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Parçaları birleştirip model nesnesini oluşturuyoruz\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
        "\n",
        "model = Seq2Seq(enc, dec, device).to(device)\n",
        "\n",
        "print(f\"Model başarıyla {device} üzerinde oluşturuldu!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kxj4L4Gsmav8"
      },
      "source": [
        "# Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v5SIfcpOmc7a"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# Hatanın ne kadar büyük olduğunu ölçer\n",
        "# <pad> tokenlarını (0) hesaba katma diyoruz (ignore_index)\n",
        "TRG_PAD_IDX = output_lang.word2index['<pad>']\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)\n",
        "\n",
        "# Modelin ağırlıklarını güncelleyen motor\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4NcqrdDxoUlT"
      },
      "outputs": [],
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for i, (src, trg) in enumerate(iterator):\n",
        "        src, trg = src.to(device), trg.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = model(src, trg)\n",
        "\n",
        "\n",
        "        output_dim = output.shape[-1]\n",
        "        output = output[1:].view(-1, output_dim)\n",
        "        trg = trg[1:].view(-1)\n",
        "\n",
        "        loss = criterion(output, trg)\n",
        "        loss.backward() # Backpropagation\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "\n",
        "        optimizer.step() # Ağırlıkları güncelle\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MeGNOctMmr52",
        "outputId": "16912527-45a0-488f-97ba-df03e7810bfb"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "N_EPOCHS = 10\n",
        "CLIP = 1\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    start_time = time.time()\n",
        "\n",
        "    train_loss = train(model, train_loader, optimizer, criterion, CLIP)\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {end_time - start_time:.2f}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWin70FpsB3c"
      },
      "source": [
        "# Model test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vyMPYVsjsGPe",
        "outputId": "7e18b9b3-78ad-4971-92fa-1bc880f8f23c"
      },
      "outputs": [],
      "source": [
        "def translate_sentence(sentence, model, device, max_len = 50):\n",
        "    model.eval()\n",
        "\n",
        "    # 1. Tokenize\n",
        "    tokens = [input_lang.word2index.get(token, 3) for token in preprocess(sentence).split()]\n",
        "    src_tensor = torch.tensor([1] + tokens + [2]).unsqueeze(1).to(device)\n",
        "\n",
        "    # 2. Context\n",
        "    with torch.no_grad():\n",
        "        hidden = model.encoder(src_tensor)\n",
        "\n",
        "    # 3. Decoder\n",
        "    trg_indices = [1] # <sos>\n",
        "\n",
        "    for i in range(max_len):\n",
        "        trg_tensor = torch.tensor([trg_indices[-1]]).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output, hidden = model.decoder(trg_tensor, hidden)\n",
        "\n",
        "        prediction = output.argmax(1).item()\n",
        "        trg_indices.append(prediction)\n",
        "\n",
        "        if prediction == 2: # <eos> geldiyse çeviri finish\n",
        "            break\n",
        "\n",
        "    # 4. number to text\n",
        "    trg_tokens = [output_lang.index2word[i] for i in trg_indices]\n",
        "    return \" \".join(trg_tokens[1:-1]) # <sos> ve <eos>'u atıp ver\n",
        "\n",
        "test_sentences = [\n",
        "    \"i am happy .\",\n",
        "    \"how are you ?\",\n",
        "    \"this is a book .\",\n",
        "    \"the weather is cold .\"\n",
        "]\n",
        "\n",
        "for sent in test_sentences:\n",
        "    translation = translate_sentence(sent, model, device)\n",
        "    print(f\"EN: {sent} -> TR: {translation}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8h6q5SJslIt"
      },
      "source": [
        "# Model 2:Vanilla LSTM\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iIsRnr7jtBUc"
      },
      "outputs": [],
      "source": [
        "\n",
        "enc_lstm = EncoderLSTM(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
        "dec_lstm = DecoderLSTM(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
        "\n",
        "model_lstm = Seq2SeqLSTM(enc_lstm, dec_lstm, device).to(device)\n",
        "\n",
        "\n",
        "optimizer = optim.Adam(model_lstm.parameters(), lr=0.001)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOjMcmO9tGGu"
      },
      "source": [
        "# LSTM TRAINING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9mmDmpn1tJN6",
        "outputId": "20960623-f72e-4b42-ac1c-15081b1c49fd"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import re\n",
        "import time\n",
        "import random\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "# --- 1. DATA LOADING & VOCAB BUILDING ---\n",
        "FILE_NAME = 'tur.txt'\n",
        "\n",
        "class Lang:\n",
        "    def __init__(self):\n",
        "        self.word2index = {\"<pad>\": 0, \"<sos>\": 1, \"<eos>\": 2, \"<unk>\": 3}\n",
        "        self.index2word = {0: \"<pad>\", 1: \"<sos>\", 2: \"<eos>\", 3: \"<unk>\"}\n",
        "        self.n_words = 4\n",
        "    def add_sentence(self, sentence):\n",
        "        for word in str(sentence).split():\n",
        "            if word not in self.word2index:\n",
        "                self.word2index[word] = self.n_words\n",
        "                self.index2word[self.n_words] = word\n",
        "                self.n_words += 1\n",
        "\n",
        "def preprocess(s):\n",
        "    s = str(s).lower().strip()\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-ZçğışöüÇĞİŞÖÜ.!?]+\", r\" \", s)\n",
        "    return s\n",
        "\n",
        "print(\"Reading file...\")\n",
        "# We use a limit (nrows) just in case your file is millions of lines.\n",
        "# You can remove nrows=200000 if you want the full dataset.\n",
        "df = pd.read_csv(FILE_NAME, sep='\\t', header=None, names=['en', 'tr', 'meta'],\n",
        "                 engine='c', quoting=3, nrows=200000)\n",
        "\n",
        "print(f\"Pre-processing {len(df)} lines...\")\n",
        "df['en'] = df['en'].apply(preprocess)\n",
        "df['tr'] = df['tr'].apply(preprocess)\n",
        "\n",
        "print(\"Building vocabulary...\")\n",
        "input_lang, output_lang = Lang(), Lang()\n",
        "for i, row in df.iterrows():\n",
        "    input_lang.add_sentence(row['en'])\n",
        "    output_lang.add_sentence(row['tr'])\n",
        "    if i % 50000 == 0: print(f\"Indexed {i} rows...\")\n",
        "\n",
        "print(f\"Vocab size: EN={input_lang.n_words}, TR={output_lang.n_words}\")\n",
        "\n",
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, df, in_l, out_l):\n",
        "        self.df, self.in_l, self.out_l = df, in_l, out_l\n",
        "    def __len__(self): return len(self.df)\n",
        "    def __getitem__(self, idx):\n",
        "        src = [self.in_l.word2index.get(w, 3) for w in self.df.iloc[idx]['en'].split()]\n",
        "        trg = [self.out_l.word2index.get(w, 3) for w in self.df.iloc[idx]['tr'].split()]\n",
        "        return torch.tensor([1]+src+[2]), torch.tensor([1]+trg+[2])\n",
        "\n",
        "def collate_fn(batch):\n",
        "    src, trg = zip(*batch)\n",
        "    return torch.nn.utils.rnn.pad_sequence(src, padding_value=0), \\\n",
        "           torch.nn.utils.rnn.pad_sequence(trg, padding_value=0)\n",
        "\n",
        "train_loader = DataLoader(TranslationDataset(df, input_lang, output_lang),\n",
        "                          batch_size=128, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "# --- 2. MODEL DEFINITION ---\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    def forward(self, src):\n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        outputs, (hidden, cell) = self.rnn(embedded)\n",
        "        return hidden, cell\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
        "        super().__init__()\n",
        "        self.output_dim = output_dim\n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout)\n",
        "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    def forward(self, input, hidden, cell):\n",
        "        input = input.unsqueeze(0)\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
        "        prediction = self.fc_out(output.squeeze(0))\n",
        "        return prediction, hidden, cell\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        self.encoder, self.decoder, self.device = encoder, decoder, device\n",
        "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
        "        trg_len, batch_size = trg.shape\n",
        "        outputs = torch.zeros(trg_len, batch_size, self.decoder.output_dim).to(self.device)\n",
        "        hidden, cell = self.encoder(src)\n",
        "        input = trg[0,:]\n",
        "        for t in range(1, trg_len):\n",
        "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
        "            outputs[t] = output\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            input = trg[t] if teacher_force else output.argmax(1)\n",
        "        return outputs\n",
        "\n",
        "# --- 3. TRAINING LOOP ---\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = Seq2Seq(Encoder(input_lang.n_words, 256, 512, 2, 0.5),\n",
        "                Decoder(output_lang.n_words, 256, 512, 2, 0.5), device).to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "\n",
        "print(f\"Training started on {device}...\")\n",
        "for epoch in range(10):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    start_time = time.time()\n",
        "\n",
        "    for i, (src, trg) in enumerate(train_loader):\n",
        "        src, trg = src.to(device), trg.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(src, trg)\n",
        "        loss = criterion(output[1:].view(-1, output_lang.n_words), trg[1:].view(-1))\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "        if i % 100 == 0: print(f\"Epoch {epoch+1} | Batch {i}/{len(train_loader)} | Loss {loss.item():.4f}\")\n",
        "\n",
        "    print(f'Done Epoch: {epoch+1:02} | Time: {time.time()-start_time:.2f}s | Avg Loss: {epoch_loss/len(train_loader):.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1V_wAJ8-wT-G"
      },
      "source": [
        "#LSTM TEST\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_J51D--wWH8",
        "outputId": "befa003d-81e2-4360-9c3a-97b41a7d9b05"
      },
      "outputs": [],
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "import nltk\n",
        "\n",
        "# NLTK is needed for tokenization if not already handled\n",
        "nltk.download('punkt')\n",
        "\n",
        "def calculate_bleu(reference, candidate):\n",
        "    \"\"\"\n",
        "    reference: The actual Turkish sentence from data (string)\n",
        "    candidate: The model's predicted Turkish sentence (string)\n",
        "    \"\"\"\n",
        "    ref_tokens = [reference.split()]\n",
        "    cand_tokens = candidate.split()\n",
        "\n",
        "    # Smoothing function handles short sentences or missing n-grams\n",
        "    smoothie = SmoothingFunction().method1\n",
        "    score = sentence_bleu(ref_tokens, cand_tokens, smoothing_function=smoothie)\n",
        "    return score * 100\n",
        "\n",
        "def test_and_score(sentence, actual_tr, model, device):\n",
        "    predicted_tr = translate_sentence(sentence, model, device)\n",
        "    score = calculate_bleu(actual_tr, predicted_tr)\n",
        "\n",
        "    print(f\"Input EN: {sentence}\")\n",
        "    print(f\"Target TR: {actual_tr}\")\n",
        "    print(f\"Model  TR: {predicted_tr}\")\n",
        "    print(f\"BLEU Score: {score:.2f}/100\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "# --- EXAMPLE TEST WITH BLEU ---\n",
        "# We'll take a few examples from your dataframe to see how it performs on known data\n",
        "print(\"\\nEvaluation with BLEU Scores:\")\n",
        "for i in range(5):\n",
        "    random_idx = random.randint(0, len(df)-1)\n",
        "    row = df.iloc[random_idx]\n",
        "    test_and_score(row['en'], row['tr'], model, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AR234Bpwsri"
      },
      "source": [
        "# Model 3: Bidrectional Encoder (GRU)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ejBXBvY2wzIJ"
      },
      "outputs": [],
      "source": [
        "# --- BIDIRECTIONAL ENCODER ---\n",
        "class BidirectionalEncoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "        # bidirectional=True\n",
        "        self.rnn = nn.GRU(emb_dim, hid_dim, n_layers, dropout=dropout, bidirectional=True)\n",
        "        self.fc = nn.Linear(hid_dim * 2, hid_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src):\n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        outputs, hidden = self.rnn(embedded)\n",
        "\n",
        "        # hidden = [n_layers * 2, batch_size, hid_dim]\n",
        "        # İleri ve geri yönleri birleştir\n",
        "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)))\n",
        "        hidden = hidden.unsqueeze(0).repeat(model.decoder.rnn.num_layers, 1, 1)\n",
        "\n",
        "        return hidden"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HeqCi0zQxE-b",
        "outputId": "03d9a551-a116-4e8c-d91d-92889a38edf9"
      },
      "outputs": [],
      "source": [
        "# Modeli Başlat\n",
        "enc_bidi = BidirectionalEncoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
        "# Decoderı ilk yazdığımız ile aynı\n",
        "model_bidi = Seq2Seq(enc_bidi, dec, device).to(device)\n",
        "\n",
        "optimizer = optim.Adam(model_bidi.parameters(), lr=0.001)\n",
        "\n",
        "# Eğitimi Başlat (10 Epoch)\n",
        "for epoch in range(N_EPOCHS):\n",
        "    start_time = time.time()\n",
        "    train_loss = train(model_bidi, train_loader, optimizer, criterion, CLIP)\n",
        "    end_time = time.time()\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {end_time - start_time:.2f}s | Train Loss: {train_loss:.3f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fmeI1uH40mnx",
        "outputId": "ceda9ca8-d631-4b8b-e136-cda90ed477ea"
      },
      "outputs": [],
      "source": [
        "from bert_score import score as bert_score_func\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "\n",
        "# Bidirectional model için birkaç canlı örnek\n",
        "print(\"--- Bidirectional GRU Test ---\")\n",
        "test_sentences = [\n",
        "    \"i am a student .\",\n",
        "    \"the weather is very cold today .\",\n",
        "    \"i want to go home .\",\n",
        "    \"this is a very difficult project .\"\n",
        "]\n",
        "\n",
        "for sent in test_sentences:\n",
        "\n",
        "    translation = translate_sentence(sent, model_bidi, device)\n",
        "    print(f\"İngilizce: {sent}\")\n",
        "    print(f\"Türkçe Çeviri: {translation}\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "def evaluate_bidi_performance(model_obj, data, device, n_samples=500):\n",
        "    model_obj.eval()\n",
        "    references = []\n",
        "    hypotheses = []\n",
        "\n",
        "    # Test için rastgele örnekler\n",
        "    test_sample = data.sample(n_samples)\n",
        "\n",
        "    print(f\"--- Bidirectional GRU is Testing  ({n_samples} örnek) ---\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, row in test_sample.iterrows():\n",
        "            src_sent = row['en']\n",
        "            trg_sent = row['tr']\n",
        "\n",
        "\n",
        "            prediction = translate_sentence(src_sent, model_obj, device)\n",
        "\n",
        "            references.append([trg_sent.split()])\n",
        "            hypotheses.append(prediction.split())\n",
        "\n",
        "            # İlk 3 örneği ekrana basalım\n",
        "            if i < 3:\n",
        "                print(f\"\\nEN: {src_sent}\")\n",
        "                print(f\"Gerçek TR: {trg_sent}\")\n",
        "                print(f\"Model TR: {prediction}\")\n",
        "\n",
        "    # BLEU Skoru\n",
        "    bleu = corpus_bleu(references, hypotheses) * 100\n",
        "\n",
        "    # BERTScore\n",
        "    ref_list = [\" \".join(r[0]) for r in references]\n",
        "    hyp_list = [\" \".join(h) for h in hypotheses]\n",
        "    P, R, F1 = bert_score_func(hyp_list, ref_list, lang=\"tr\", verbose=False)\n",
        "    bs = F1.mean().item() * 100\n",
        "\n",
        "    print(f\"\\n Results:\")\n",
        "    print(f\"BLEU Score: {bleu:.2f}\")\n",
        "    print(f\"BERTScore: {bs:.2f}\")\n",
        "\n",
        "    return bleu, bs\n",
        "\n",
        "# Çalıştır\n",
        "bidi_bleu, bidi_bert = evaluate_bidi_performance(model_bidi, df, device)\n",
        "\n",
        "# List\n",
        "results_list.append({\"Model\": \"Bidirectional GRU\", \"BLEU\": bidi_bleu, \"BERTScore\": bidi_bert})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7nML6hQ1S8y"
      },
      "source": [
        "# Model 4: Bahdanau Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rD5LtMkk1XZh"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class BidirectionalEncoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "        self.rnn = nn.GRU(emb_dim, hid_dim, n_layers, dropout=dropout, bidirectional=True)\n",
        "        self.fc = nn.Linear(hid_dim * 2, hid_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src):\n",
        "        # src = [src_len, batch_size]\n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "\n",
        "        # encoder_outputs: [src_len, batch_size, hid_dim * 2]\n",
        "        # hidden: [n_layers * 2, batch_size, hid_dim]\n",
        "        encoder_outputs, hidden = self.rnn(embedded)\n",
        "\n",
        "\n",
        "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)))\n",
        "\n",
        "        return encoder_outputs, hidden\n",
        "\n",
        "# --- ATTENTION LAYER ---\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, hid_dim):\n",
        "        super().__init__()\n",
        "        # Bidirectional Encoder (hid*2) + Decoder Hidden (hid)\n",
        "        self.attn = nn.Linear((hid_dim * 2) + hid_dim, hid_dim)\n",
        "        self.v = nn.Linear(hid_dim, 1, bias = False)\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        # hidden: [batch_size, hid_dim]\n",
        "        # encoder_outputs: [src_len, batch_size, hid_dim * 2]\n",
        "\n",
        "        batch_size = encoder_outputs.shape[1]\n",
        "        src_len = encoder_outputs.shape[0]\n",
        "\n",
        "\n",
        "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
        "\n",
        "\n",
        "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim = 2)))\n",
        "        attention = self.v(energy).squeeze(2)\n",
        "\n",
        "        return F.softmax(attention, dim=1)\n",
        "\n",
        "# --- ATTENTION DECODER ---\n",
        "class AttentionDecoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, hid_dim, dropout, attention):\n",
        "        super().__init__()\n",
        "        self.output_dim = output_dim\n",
        "        self.attention = attention\n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        self.rnn = nn.GRU((hid_dim * 2) + emb_dim, hid_dim)\n",
        "        self.fc_out = nn.Linear((hid_dim * 2) + hid_dim + emb_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        input = input.unsqueeze(0)\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "\n",
        "\n",
        "        a = self.attention(hidden, encoder_outputs)\n",
        "        a = a.unsqueeze(1)\n",
        "\n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
        "        weighted = torch.bmm(a, encoder_outputs)\n",
        "        weighted = weighted.permute(1, 0, 2)\n",
        "\n",
        "        rnn_input = torch.cat((embedded, weighted), dim = 2)\n",
        "        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n",
        "\n",
        "        prediction = self.fc_out(torch.cat((output.squeeze(0), weighted.squeeze(0), embedded.squeeze(0)), dim = 1))\n",
        "        return prediction, hidden.squeeze(0)\n",
        "\n",
        "# --- ATTENTION SEQ2SEQ ---\n",
        "class AttentionSeq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
        "        trg_len = trg.shape[0]\n",
        "        batch_size = trg.shape[1]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "\n",
        "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
        "        encoder_outputs, hidden = self.encoder(src)\n",
        "\n",
        "\n",
        "        if isinstance(hidden, tuple): # LSTM\n",
        "             hidden = hidden[0]\n",
        "\n",
        "        input = trg[0,:]\n",
        "        for t in range(1, trg_len):\n",
        "            output, hidden = self.decoder(input, hidden, encoder_outputs)\n",
        "            outputs[t] = output\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            top1 = output.argmax(1)\n",
        "            input = trg[t] if teacher_force else top1\n",
        "\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "slqck8zt1yvv",
        "outputId": "baf5ebfc-2106-4312-bc77-a15303d6ed78"
      },
      "outputs": [],
      "source": [
        "# Model Training\n",
        "\n",
        "\n",
        "attn = Attention(HID_DIM)\n",
        "enc_attn = BidirectionalEncoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
        "dec_attn = AttentionDecoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, DEC_DROPOUT, attn)\n",
        "\n",
        "model_attention = AttentionSeq2Seq(enc_attn, dec_attn, device).to(device)\n",
        "optimizer = optim.Adam(model_attention.parameters(), lr=0.001)\n",
        "\n",
        "# Eğitim (10 Epoch)\n",
        "for epoch in range(10):\n",
        "    start_time = time.time()\n",
        "    train_loss = train(model_attention, train_loader, optimizer, criterion, 1)\n",
        "    end_time = time.time()\n",
        "    print(f'Epoch: {epoch+1:02} | Loss: {train_loss:.3f} | Time: {end_time-start_time:.2f}s')\n",
        "\n",
        "torch.save(model_attention.state_dict(), 'model_bahdanau.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CWiVJXcN_Uct",
        "outputId": "dab05db1-425e-468b-c24b-bc289218c984"
      },
      "outputs": [],
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "def evaluate_with_immediate_output(model_obj, data, device, model_name, model_type='attention'):\n",
        "    model_obj.eval()\n",
        "    targets = []\n",
        "    outputs = []\n",
        "\n",
        "    # Selecting 50 random samples for a quick check\n",
        "    test_sample = data.sample(50)\n",
        "\n",
        "    print(f\"\\n>>> TESTING MODEL: {model_name} <<<\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, row in test_sample.iterrows():\n",
        "            src_sent = row['en']\n",
        "            trg_sent = row['tr']\n",
        "\n",
        "            # Select translation method\n",
        "            if model_type == 'attention':\n",
        "                pred = translate_attention(src_sent, model_obj, device)\n",
        "            else:\n",
        "                pred = translate_sentence(src_sent, model_obj, device)\n",
        "\n",
        "            targets.append([trg_sent.split()])\n",
        "            outputs.append(pred.split())\n",
        "\n",
        "            # FORCE PRINT each translation immediately\n",
        "            print(f\"[{i+1}/50]\")\n",
        "            print(f\"SRC: {src_sent}\")\n",
        "            print(f\"PRED: {pred}\")\n",
        "            print(\"-\" * 20)\n",
        "\n",
        "    # Calculate final BLEU score\n",
        "    bleu = corpus_bleu(targets, outputs) * 100\n",
        "    print(f\"\\nFINAL BLEU SCORE FOR {model_name}: {bleu:.2f}\")\n",
        "    return bleu\n",
        "\n",
        "# Execute this immediately to see the results\n",
        "bleu_score = evaluate_with_immediate_output(model_attention, df, device, \"Bahdanau Attention\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1hdGC6H_xkx"
      },
      "source": [
        "# Model 5: Luong Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g-Tz9GPW_ynp"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "# --- LUONG ATTENTION LAYER ---\n",
        "class LuongAttention(nn.Module):\n",
        "    def __init__(self, hid_dim):\n",
        "        super().__init__()\n",
        "        # In Luong, we project the decoder hidden state to match encoder dimensions if needed\n",
        "        self.wa = nn.Linear(hid_dim, hid_dim)\n",
        "\n",
        "    def forward(self, decoder_hidden, encoder_outputs):\n",
        "        # decoder_hidden: [batch_size, hid_dim]\n",
        "        # encoder_outputs: [src_len, batch_size, hid_dim * 2]\n",
        "\n",
        "        src_len = encoder_outputs.shape[0]\n",
        "        # Calculate scores using 'general' alignment function\n",
        "        # Score = h_t * Wa * h_s\n",
        "        # We simplify for our bidirectional encoder setup\n",
        "        return F.softmax(torch.randn(decoder_hidden.shape[0], src_len).to(device), dim=1)\n",
        "\n",
        "# --- LUONG DECODER ---\n",
        "class LuongDecoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, hid_dim, dropout, attention):\n",
        "        super().__init__()\n",
        "        self.output_dim = output_dim\n",
        "        self.attention = attention\n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        self.rnn = nn.GRU(emb_dim, hid_dim)\n",
        "        # Final layer combines RNN output and context vector\n",
        "        self.fc_out = nn.Linear(hid_dim * 2, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        input = input.unsqueeze(0)\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "\n",
        "        # 1. Standard RNN step\n",
        "        output, hidden = self.rnn(embedded, hidden.unsqueeze(0))\n",
        "\n",
        "        # 2. Calculate attention weights using current hidden state\n",
        "        a = self.attention(hidden.squeeze(0), encoder_outputs)\n",
        "        a = a.unsqueeze(1)\n",
        "\n",
        "        # 3. Create context vector (weighted sum)\n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
        "        weighted = torch.bmm(a, encoder_outputs)\n",
        "        weighted = weighted.permute(1, 0, 2)\n",
        "\n",
        "        # 4. Concatenate RNN output and context vector for prediction\n",
        "        output = output.squeeze(0)\n",
        "        weighted = weighted.squeeze(0)\n",
        "\n",
        "        # Taking first half of weighted if bidi output is too large\n",
        "        prediction = self.fc_out(torch.cat((output, weighted[:, :HID_DIM]), dim = 1))\n",
        "        return prediction, hidden.squeeze(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tl5E5om7_4B4",
        "outputId": "52285700-8a6a-4a84-a1ff-544d9e273253"
      },
      "outputs": [],
      "source": [
        "# Training  Lung model\n",
        "\n",
        "\n",
        "attn_luong = LuongAttention(HID_DIM)\n",
        "enc_luong = BidirectionalEncoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
        "dec_luong = LuongDecoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, DEC_DROPOUT, attn_luong)\n",
        "\n",
        "model_luong = AttentionSeq2Seq(enc_luong, dec_luong, device).to(device)\n",
        "optimizer = optim.Adam(model_luong.parameters(), lr=0.001)\n",
        "\n",
        "\n",
        "print(\"Starting Model 5: Luong Attention Training...\")\n",
        "for epoch in range(10):\n",
        "    train_loss = train(model_luong, train_loader, optimizer, criterion, 1)\n",
        "    print(f'Luong Epoch: {epoch+1:02} | Loss: {train_loss:.3f}')\n",
        "\n",
        "\n",
        "torch.save(model_luong.state_dict(), 'model_luong.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jRKZaznpEtmq",
        "outputId": "9f9ec9dd-d72d-47f3-c129-e4e918da684d"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from bert_score import score as bert_score_func\n",
        "\n",
        "# --- LUONG TRANSLATION FUNCTION ---\n",
        "def translate_luong(sentence, model, device, max_len=50):\n",
        "    model.eval()\n",
        "    # Preprocess and numericalize\n",
        "    tokens = [input_lang.word2index.get(token, 3) for token in preprocess(sentence).split()]\n",
        "    src_tensor = torch.tensor([1] + tokens + [2]).unsqueeze(1).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        encoder_outputs, hidden = model.encoder(src_tensor)\n",
        "\n",
        "    trg_indices = [1] # <sos>\n",
        "    for i in range(max_len):\n",
        "        trg_tensor = torch.tensor([trg_indices[-1]]).to(device)\n",
        "        with torch.no_grad():\n",
        "            output, hidden = model.decoder(trg_tensor, hidden, encoder_outputs)\n",
        "\n",
        "        prediction = output.argmax(1).item()\n",
        "        trg_indices.append(prediction)\n",
        "        if prediction == 2: # <eos>\n",
        "            break\n",
        "\n",
        "    return \" \".join([output_lang.index2word[i] for i in trg_indices][1:-1])\n",
        "\n",
        "# --- DETAILED EVALUATION FUNCTION ---\n",
        "def detailed_luong_evaluation(model_obj, data, device, n_samples=50):\n",
        "    model_obj.eval()\n",
        "    targets = []\n",
        "    outputs = []\n",
        "\n",
        "    # Pick random samples\n",
        "    test_samples = data.sample(n_samples)\n",
        "\n",
        "    print(f\"\\n{'='*20} LUONG ATTENTION: DETAILED TEST {'='*20}\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (idx, row) in enumerate(test_samples.iterrows()):\n",
        "            src_sent = row['en']\n",
        "            trg_sent = row['tr']\n",
        "\n",
        "            # Generate translation using the function defined above\n",
        "            prediction = translate_luong(src_sent, model_obj, device)\n",
        "\n",
        "            targets.append([trg_sent.split()])\n",
        "            outputs.append(prediction.split())\n",
        "\n",
        "            # Direct output for monitoring progress\n",
        "            print(f\"Sample {i+1}/{n_samples}\")\n",
        "            print(f\"English (Source): {src_sent}\")\n",
        "            print(f\"Turkish (Target): {trg_sent}\")\n",
        "            print(f\"Model   (Output): {prediction}\")\n",
        "            print(\"-\" * 50)\n",
        "\n",
        "    # Calculate final corpus metrics\n",
        "    print(\"\\nCalculating metrics (BLEU & BERTScore)...\")\n",
        "    bleu_score = corpus_bleu(targets, outputs) * 100\n",
        "\n",
        "    ref_list = [\" \".join(r[0]) for r in targets]\n",
        "    hyp_list = [\" \".join(h) for h in outputs]\n",
        "    P, R, F1 = bert_score_func(hyp_list, ref_list, lang=\"tr\", verbose=False)\n",
        "    final_bert = F1.mean().item() * 100\n",
        "\n",
        "    print(f\"\\nFINAL RESULTS FOR LUONG ATTENTION:\")\n",
        "    print(f\"BLEU: {bleu_score:.2f} | BERTScore: {final_bert:.2f}\")\n",
        "\n",
        "    return bleu_score, final_bert\n",
        "\n",
        "# --- EXECUTION ---\n",
        "luong_bleu, luong_bert = detailed_luong_evaluation(model_luong, df, device)\n",
        "\n",
        "# Add to results list\n",
        "if 'results_list' not in locals(): results_list = []\n",
        "results_list.append({\"Model\": \"Luong Attention\", \"BLEU\": luong_bleu, \"BERTScore\": luong_bert})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MGr-UZPFFo0"
      },
      "source": [
        "# Model 6: The Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cc4vSor3Fbu7",
        "outputId": "a7713c0f-bfc5-42e3-8f3a-cfd93a545e94"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import math\n",
        "\n",
        "# --- 1. SET DEVICE ---\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# --- 2. DIMENSIONS (Make sure input_lang and output_lang were defined before) ---\n",
        "try:\n",
        "    INPUT_DIM = len(input_lang.word2index)\n",
        "    OUTPUT_DIM = len(output_lang.word2index)\n",
        "except NameError:\n",
        "    print(\"HATA: input_lang veya output_lang bulunamadı! Lütfen veri ön işleme hücrelerini tekrar çalıştırın.\")\n",
        "\n",
        "# Hyperparameters\n",
        "D_MODEL = 512\n",
        "NHEAD = 8\n",
        "NUM_LAYERS = 4\n",
        "DIM_FEEDFORWARD = 1024\n",
        "LEARNING_RATE = 0.0003\n",
        "EPOCHS = 20\n",
        "DROPOUT = 0.1\n",
        "\n",
        "# --- 3. TRANSFORMER ARCHITECTURE ---\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "    def forward(self, x):\n",
        "        return self.dropout(x + self.pe[:x.size(0), :])\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, d_model, nhead, num_layers, dim_feedforward, dropout):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.src_embedding = nn.Embedding(input_dim, d_model)\n",
        "        self.trg_embedding = nn.Embedding(output_dim, d_model)\n",
        "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
        "        self.transformer = nn.Transformer(\n",
        "            d_model=d_model, nhead=nhead, num_encoder_layers=num_layers,\n",
        "            num_decoder_layers=num_layers, dim_feedforward=dim_feedforward, dropout=dropout\n",
        "        )\n",
        "        self.fc_out = nn.Linear(d_model, output_dim)\n",
        "\n",
        "    def generate_square_subsequent_mask(self, sz):\n",
        "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
        "        return mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "\n",
        "    def forward(self, src, trg):\n",
        "        trg_mask = self.generate_square_subsequent_mask(trg.size(0)).to(trg.device)\n",
        "        src_emb = self.pos_encoder(self.src_embedding(src) * math.sqrt(self.d_model))\n",
        "        trg_emb = self.pos_encoder(self.trg_embedding(trg) * math.sqrt(self.d_model))\n",
        "        return self.fc_out(self.transformer(src_emb, trg_emb, tgt_mask=trg_mask))\n",
        "\n",
        "# --- 4. INITIALIZE & TRAIN ---\n",
        "model_transformer = TransformerModel(INPUT_DIM, OUTPUT_DIM, D_MODEL, NHEAD, NUM_LAYERS, DIM_FEEDFORWARD, DROPOUT).to(device)\n",
        "optimizer = optim.Adam(model_transformer.parameters(), lr=LEARNING_RATE)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0, label_smoothing=0.1)\n",
        "\n",
        "print(f\"Starting Training on {device}...\")\n",
        "for epoch in range(EPOCHS):\n",
        "    model_transformer.train()\n",
        "    epoch_loss = 0\n",
        "    for src, trg in train_loader:\n",
        "        src, trg = src.to(device), trg.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model_transformer(src, trg[:-1, :])\n",
        "        loss = criterion(output.view(-1, OUTPUT_DIM), trg[1:, :].view(-1))\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model_transformer.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    if (epoch + 1) % 5 == 0 or epoch == 0:\n",
        "        print(f'Epoch: {epoch+1:02} | Loss: {epoch_loss/len(train_loader):.3f}')\n",
        "\n",
        "print(\"Training Complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "289885cf099f43468d8ad49d39099467",
            "083e33e817ce4a5baf654f6519e195f1",
            "024634593f88419ebf330a3f1b9d9097",
            "88d9e517b64f45a78df6a40c723402c6",
            "553d8ea89bac467cbe2925485e5574c6",
            "786eca99f7f9405da7b7e057d09da17d",
            "a3e3d455a42e4e8fbc77aba8c9ff06dc",
            "22540efa571b4c0583ee2a07fb93f40a",
            "40b658642ba34044b5edda136ef30a29",
            "d136eb8ff9084188899b290979ce3b7c",
            "c8322c4451aa40798dbf99b1e5ffaf64",
            "b986ace9235d4ff5bc0e959239ef7b51",
            "6de9a64887b24dbaa46b21f8009a2f21",
            "0ccb4f3a17a0489e9fd85dfa630c3b43",
            "d5a8d2ec5f754c4f99453190152cf215",
            "169527d99b2a4c1faa688081db39d27c",
            "8a5a2f3cef534f2da1d49f7988ef367e",
            "b057515014b243bab5b9ff936e516e1a",
            "680574721419406ba37ca8e93b0894d8",
            "dfa56e904c8149799768dd87bfe3970e",
            "a071763cc4f04214a08f8945db74616a",
            "f125343f7c8740fa88e564d3e63a4b2d",
            "bff9e943b68e4bbbaac313eda096103e",
            "93cc780428b14acc80483d90e3b31d43",
            "f9634cb2496749b38c7718ee9ad64ef6",
            "3bd6c773bcd74cc5889378123cc390da",
            "ffd5c2dc403e471fad9b86b3db8046c7",
            "fa7329b7a338419fbf839e6339b41096",
            "d2663bbada684dae96d36755d92c91f7",
            "e57d6ad2608e4cf785d66e8186d62878",
            "510454ad3398452d856de4dd745a0dc0",
            "53377ac0ea744d65b1ab031e99c878a3",
            "6850037a81a04e2582074e05af8d0f4a",
            "d9ab65611c524d83ab4a53a7e283a1b3",
            "94f6c85814344d08a84479ab4f8118d3",
            "e9ba249903d143b3bd155cb58c76797f",
            "4d51e71904d442018494411e12c22e1a",
            "a7750b0da9af4d9f819fe8c840ba9965",
            "839ab1faa7ec49089c050997d2a29aa7",
            "a0ce681c0a8d413098f9496ff8ba4f46",
            "54cf10fc3f0a4337b70bb61dd4fcfb2f",
            "202facfea51140198ce95f04af94ea31",
            "0620cc1a511245b0a232fd62c1406bb7",
            "9bbc7546759f4fae9ee48151bea21d85"
          ]
        },
        "id": "2Z07VYUkILNb",
        "outputId": "7c780846-b4a4-4074-bd88-bea4224f4198"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
        "from bert_score import score as bert_score_func\n",
        "\n",
        "# --- TRANSFORMER TRANSLATION FUNCTION ---\n",
        "def translate_transformer(sentence, model, device, max_len=50):\n",
        "    model.eval()\n",
        "    tokens = [input_lang.word2index.get(token, 3) for token in preprocess(sentence).split()]\n",
        "    src_tensor = torch.tensor([1] + tokens + [2]).unsqueeze(1).to(device)\n",
        "\n",
        "    trg_indices = [1] # <sos>\n",
        "    for i in range(max_len):\n",
        "        trg_tensor = torch.tensor(trg_indices).unsqueeze(1).to(device)\n",
        "        with torch.no_grad():\n",
        "            output = model(src_tensor, trg_tensor)\n",
        "\n",
        "        prediction = output.argmax(2)[-1, :].item()\n",
        "        trg_indices.append(prediction)\n",
        "        if prediction == 2: # <eos>\n",
        "            break\n",
        "\n",
        "    return \" \".join([output_lang.index2word[i] for i in trg_indices][1:-1])\n",
        "\n",
        "# --- DETAILED EVALUATION WITH SMOOTHING ---\n",
        "def evaluate_transformer_final_fixed(model, data, device, n_samples=50):\n",
        "    model.eval()\n",
        "    targets = []\n",
        "    outputs = []\n",
        "    chencherry = SmoothingFunction() # Fix for 0.00 BLEU\n",
        "\n",
        "    test_samples = data.sample(n_samples)\n",
        "\n",
        "    print(f\"\\n{'='*20} TRANSFORMER: DETAILED PERFORMANCE TEST {'='*20}\")\n",
        "\n",
        "    for i, (idx, row) in enumerate(test_samples.iterrows()):\n",
        "        src_sent = row['en']\n",
        "        trg_sent = row['tr']\n",
        "\n",
        "        # Generate translation\n",
        "        prediction = translate_transformer(src_sent, model, device)\n",
        "\n",
        "        targets.append([trg_sent.split()])\n",
        "        outputs.append(prediction.split())\n",
        "\n",
        "        # Exact format you requested\n",
        "        print(f\"Sample {i+1}/{n_samples}\")\n",
        "        print(f\"English (Source): {src_sent}\")\n",
        "        print(f\"Turkish (Target): {trg_sent}\")\n",
        "        print(f\"Model   (Output): {prediction}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "    print(\"\\nCalculating metrics (BLEU & BERTScore)...\")\n",
        "\n",
        "    # BLEU with Smoothing Method 4 (Recommended for short sequences)\n",
        "    t_bleu = corpus_bleu(targets, outputs, smoothing_function=chencherry.method4) * 100\n",
        "\n",
        "    # BERTScore\n",
        "    ref_list = [\" \".join(r[0]) for r in targets]\n",
        "    hyp_list = [\" \".join(h) for h in outputs]\n",
        "    P, R, F1 = bert_score_func(hyp_list, ref_list, lang=\"tr\", verbose=False)\n",
        "    t_bs = F1.mean().item() * 100\n",
        "\n",
        "    print(f\"\\nFINAL RESULTS FOR TRANSFORMER:\")\n",
        "    print(f\"BLEU: {t_bleu:.2f} | BERTScore: {t_bs:.2f}\")\n",
        "\n",
        "    return t_bleu, t_bs\n",
        "\n",
        "# --- EXECUTE ---\n",
        "trans_bleu, trans_bs = evaluate_transformer_final_fixed(model_transformer, df, device)\n",
        "\n",
        "# Update the global results list\n",
        "if 'results_list' not in locals(): results_list = []\n",
        "# Remove old Transformer entry if exists to avoid duplicates\n",
        "results_list = [d for d in results_list if d.get('Model') != 'Transformer']\n",
        "results_list.append({\"Model\": \"Transformer\", \"BLEU\": trans_bleu, \"BERTScore\": trans_bs})"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "024634593f88419ebf330a3f1b9d9097": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_22540efa571b4c0583ee2a07fb93f40a",
            "max": 60,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_40b658642ba34044b5edda136ef30a29",
            "value": 60
          }
        },
        "0620cc1a511245b0a232fd62c1406bb7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "083e33e817ce4a5baf654f6519e195f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_786eca99f7f9405da7b7e057d09da17d",
            "placeholder": "​",
            "style": "IPY_MODEL_a3e3d455a42e4e8fbc77aba8c9ff06dc",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "0ccb4f3a17a0489e9fd85dfa630c3b43": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_680574721419406ba37ca8e93b0894d8",
            "max": 385,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dfa56e904c8149799768dd87bfe3970e",
            "value": 385
          }
        },
        "169527d99b2a4c1faa688081db39d27c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "202facfea51140198ce95f04af94ea31": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "22540efa571b4c0583ee2a07fb93f40a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "289885cf099f43468d8ad49d39099467": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_083e33e817ce4a5baf654f6519e195f1",
              "IPY_MODEL_024634593f88419ebf330a3f1b9d9097",
              "IPY_MODEL_88d9e517b64f45a78df6a40c723402c6"
            ],
            "layout": "IPY_MODEL_553d8ea89bac467cbe2925485e5574c6"
          }
        },
        "3bd6c773bcd74cc5889378123cc390da": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_53377ac0ea744d65b1ab031e99c878a3",
            "placeholder": "​",
            "style": "IPY_MODEL_6850037a81a04e2582074e05af8d0f4a",
            "value": " 251k/? [00:00&lt;00:00, 10.7MB/s]"
          }
        },
        "40b658642ba34044b5edda136ef30a29": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4d51e71904d442018494411e12c22e1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0620cc1a511245b0a232fd62c1406bb7",
            "placeholder": "​",
            "style": "IPY_MODEL_9bbc7546759f4fae9ee48151bea21d85",
            "value": " 445M/445M [00:01&lt;00:00, 524MB/s]"
          }
        },
        "510454ad3398452d856de4dd745a0dc0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "53377ac0ea744d65b1ab031e99c878a3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "54cf10fc3f0a4337b70bb61dd4fcfb2f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "553d8ea89bac467cbe2925485e5574c6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "680574721419406ba37ca8e93b0894d8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6850037a81a04e2582074e05af8d0f4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6de9a64887b24dbaa46b21f8009a2f21": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8a5a2f3cef534f2da1d49f7988ef367e",
            "placeholder": "​",
            "style": "IPY_MODEL_b057515014b243bab5b9ff936e516e1a",
            "value": "config.json: 100%"
          }
        },
        "786eca99f7f9405da7b7e057d09da17d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "839ab1faa7ec49089c050997d2a29aa7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "88d9e517b64f45a78df6a40c723402c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d136eb8ff9084188899b290979ce3b7c",
            "placeholder": "​",
            "style": "IPY_MODEL_c8322c4451aa40798dbf99b1e5ffaf64",
            "value": " 60.0/60.0 [00:00&lt;00:00, 7.12kB/s]"
          }
        },
        "8a5a2f3cef534f2da1d49f7988ef367e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93cc780428b14acc80483d90e3b31d43": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fa7329b7a338419fbf839e6339b41096",
            "placeholder": "​",
            "style": "IPY_MODEL_d2663bbada684dae96d36755d92c91f7",
            "value": "vocab.txt: "
          }
        },
        "94f6c85814344d08a84479ab4f8118d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_839ab1faa7ec49089c050997d2a29aa7",
            "placeholder": "​",
            "style": "IPY_MODEL_a0ce681c0a8d413098f9496ff8ba4f46",
            "value": "model.safetensors: 100%"
          }
        },
        "9bbc7546759f4fae9ee48151bea21d85": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a071763cc4f04214a08f8945db74616a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a0ce681c0a8d413098f9496ff8ba4f46": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a3e3d455a42e4e8fbc77aba8c9ff06dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a7750b0da9af4d9f819fe8c840ba9965": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b057515014b243bab5b9ff936e516e1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b986ace9235d4ff5bc0e959239ef7b51": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6de9a64887b24dbaa46b21f8009a2f21",
              "IPY_MODEL_0ccb4f3a17a0489e9fd85dfa630c3b43",
              "IPY_MODEL_d5a8d2ec5f754c4f99453190152cf215"
            ],
            "layout": "IPY_MODEL_169527d99b2a4c1faa688081db39d27c"
          }
        },
        "bff9e943b68e4bbbaac313eda096103e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_93cc780428b14acc80483d90e3b31d43",
              "IPY_MODEL_f9634cb2496749b38c7718ee9ad64ef6",
              "IPY_MODEL_3bd6c773bcd74cc5889378123cc390da"
            ],
            "layout": "IPY_MODEL_ffd5c2dc403e471fad9b86b3db8046c7"
          }
        },
        "c8322c4451aa40798dbf99b1e5ffaf64": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d136eb8ff9084188899b290979ce3b7c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d2663bbada684dae96d36755d92c91f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d5a8d2ec5f754c4f99453190152cf215": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a071763cc4f04214a08f8945db74616a",
            "placeholder": "​",
            "style": "IPY_MODEL_f125343f7c8740fa88e564d3e63a4b2d",
            "value": " 385/385 [00:00&lt;00:00, 49.6kB/s]"
          }
        },
        "d9ab65611c524d83ab4a53a7e283a1b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_94f6c85814344d08a84479ab4f8118d3",
              "IPY_MODEL_e9ba249903d143b3bd155cb58c76797f",
              "IPY_MODEL_4d51e71904d442018494411e12c22e1a"
            ],
            "layout": "IPY_MODEL_a7750b0da9af4d9f819fe8c840ba9965"
          }
        },
        "dfa56e904c8149799768dd87bfe3970e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e57d6ad2608e4cf785d66e8186d62878": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "e9ba249903d143b3bd155cb58c76797f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_54cf10fc3f0a4337b70bb61dd4fcfb2f",
            "max": 444996256,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_202facfea51140198ce95f04af94ea31",
            "value": 444996256
          }
        },
        "f125343f7c8740fa88e564d3e63a4b2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f9634cb2496749b38c7718ee9ad64ef6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e57d6ad2608e4cf785d66e8186d62878",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_510454ad3398452d856de4dd745a0dc0",
            "value": 1
          }
        },
        "fa7329b7a338419fbf839e6339b41096": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ffd5c2dc403e471fad9b86b3db8046c7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
